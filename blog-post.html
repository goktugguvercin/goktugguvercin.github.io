<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Blog Post - Goktug Guvercin</title>
    
    <!-- MathJax for LaTeX rendering -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true
            }
        };
    </script>
    
    <style>
        body {
            background-color: #f5ebe0;
            margin: 0;
            min-height: 100vh;
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
        }

        .navigation {
            background-color: #d4a373;
            padding: 1rem 0;
            width: 100%;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
            position: fixed;
            top: 0;
            left: 0;
            z-index: 1000;
        }

        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            display: flex;
            justify-content: center;
            gap: 2rem;
            padding: 0 2rem;
        }

        .nav-item {
            text-decoration: none;
            color: #2c2c2c;
            font-size: 1.1rem;
            font-weight: 500;
            padding: 0.5rem 1.5rem;
            border-radius: 8px;
            transition: all 0.3s ease;
            background-color: transparent;
        }

        .nav-item:hover {
            background-color: #c4935e;
            transform: translateY(-2px);
        }

        .nav-item.active {
            background-color: #1a73e8;
            color: white;
        }

        .post-container {
            max-width: 800px;
            margin: 100px auto 2rem;
            padding: 0 2rem;
        }

        .back-link {
            display: inline-flex;
            align-items: center;
            color: #2c2c2c;
            text-decoration: none;
            margin-bottom: 2rem;
            font-weight: 500;
            transition: color 0.3s ease;
        }

        .back-link:hover {
            color: #1a73e8;
        }

        .back-link::before {
            content: "←";
            margin-right: 0.5rem;
            font-size: 1.2rem;
        }

        .post-header {
            margin-bottom: 2rem;
        }

        .post-date {
            color: #888;
            font-size: 0.95rem;
            margin-bottom: 1rem;
        }

        .post-title {
            font-size: 2.5rem;
            color: #1a1a1a;
            margin: 0 0 1rem 0;
            line-height: 1.2;
        }

        .post-content {
            background: linear-gradient(135deg, #faf7f2 0%, #e8dcc8 100%);
            padding: 3rem;
            border-radius: 16px;
            box-shadow: 0 2px 12px rgba(0, 0, 0, 0.08);
            line-height: 1.8;
            color: #333;
        }

        .post-content h2 {
            color: #1a1a1a;
            margin-top: 2rem;
            margin-bottom: 1rem;
            font-size: 1.8rem;
        }

        .post-content h3 {
            color: #2c2c2c;
            margin-top: 1.5rem;
            margin-bottom: 0.75rem;
            font-size: 1.4rem;
        }

        .post-content p {
            margin-bottom: 1.2rem;
        }

        .post-content code {
            background-color: #f5f5f5;
            padding: 0.2rem 0.4rem;
            border-radius: 4px;
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
        }

        .post-content pre {
            background-color: #f5f5f5;
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin: 1.5rem 0;
        }

        .post-content pre code {
            background-color: transparent;
            padding: 0;
        }

        .post-content img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 1.5rem 0;
        }

        .post-content ul, .post-content ol {
            margin: 1rem 0;
            padding-left: 2rem;
        }

        .post-content li {
            margin-bottom: 0.5rem;
        }

        .post-tags {
            display: flex;
            gap: 0.5rem;
            margin-top: 2rem;
            flex-wrap: wrap;
        }

        .tag {
            background-color: #d4a373;
            color: #2c2c2c;
            padding: 0.4rem 1rem;
            border-radius: 16px;
            font-size: 0.9rem;
        }
    </style>
</head>
<body>
    <nav class="navigation">
        <div class="nav-container">
            <a href="index.html" class="nav-item">About</a>
            <a href="projects.html" class="nav-item">Projects</a>
            <a href="blog.html" class="nav-item active">Blog</a>
        </div>
    </nav>

    <div class="post-container">
        <a href="blog.html" class="back-link">Back to Blog</a>
        
        <article>
            <header class="post-header">
                <div class="post-date" id="postDate">Loading...</div>
                <h1 class="post-title" id="postTitle">Loading...</h1>
            </header>

            <div class="post-content" id="postContent">
                <!-- Blog post content will be loaded here -->
            </div>

            <div class="post-tags" id="postTags">
                <!-- Tags will be loaded here -->
            </div>
        </article>
    </div>

    <script>
        // Blog posts content database
        const blogPostsContent = {
            "transformers-attention-mechanism": {
                title: "Transformers and Attention Mechanism",
                date: "November 10, 2025",
                tags: ["Machine Learning", "NLP", "Transformers"],
                content: `
                    <h2>Introduction</h2>
                    <p>The transformer architecture has revolutionized natural language processing and many other domains of machine learning. In this post, we'll explore the key innovation that makes transformers so powerful: the attention mechanism.</p>
                    
                    <h2>What is Attention?</h2>
                    <p>At its core, attention allows a model to focus on relevant parts of the input when processing each element. Unlike traditional sequential models like RNNs, transformers can attend to any part of the sequence directly.</p>
                    
                    <h3>The Attention Formula</h3>
                    <p>The scaled dot-product attention is computed as:</p>
                    <p>$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$</p>
                    
                    <p>Where $d_k$ is the dimension of the key vectors. The division by $\\sqrt{d_k}$ prevents the dot products from becoming too large.</p>
                    
                    <h2>Key Components</h2>
                    <ul>
                        <li><strong>Query (Q):</strong> What we're looking for</li>
                        <li><strong>Key (K):</strong> What we're matching against</li>
                        <li><strong>Value (V):</strong> The actual information we retrieve</li>
                    </ul>
                    
                    <h3>Multi-Head Attention</h3>
                    <p>Instead of performing a single attention function, multi-head attention runs $h$ attention mechanisms in parallel:</p>
                    <p>$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$</p>
                    <p>where each head is: $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$</p>
                    
                    <h2>Example: Adding an Image</h2>
                    <p>You can easily add images to your blog posts. Here's the syntax:</p>
                    <pre><code>&lt;img src="path/to/your/image.png" alt="Description"&gt;</code></pre>
                    
                    <h2>Conclusion</h2>
                    <p>Understanding the attention mechanism is crucial for working with modern NLP models. This foundational concept enables transformers to process sequences efficiently and effectively.</p>
                `
            },
            "sampling-frequency-audio": {
                title: "Sampling and Frequency in Audio Models",
                date: "November 5, 2025",
                tags: ["Audio Processing", "Signal Processing", "Digital Audio"],
                content: `
                    <h2>From Analog to Digital Audio</h2>
                    <p>Sound in the real world comprises pressure waves in the air. It is smooth, and continuously changes. A microphone converts those pressure waves into the voltage that changes smoothly over time. This electrical signal is called analog audio, and its waveform is roughly proportional to the original sound pressure.</p>
                    
                    <p>This analog signal is then turned into a digital audio record by an A/D converter. These converters look at analog signal many times per second, measures how high or low that voltage is and converts measured voltage (amplitude) into a number. In the final stage, these values are quantized into the nearest digital value, as its bit depth allows. The whole process is named as sampling, and the number of samples per second refers to the sample rate (Hz). Higher sampling rate captures higher frequencies and more detail about the original signal.</p>
                    
                    <p>At the end, this digital audio is stored in files such as WAV or AIFF, which typically contain uncompressed PCM audio. MP3 format also stores digital audio, but it uses lossy compression to reduce file size by discarding some audio information.</p>
                    
                    <h2>Understanding Frequency and Cycles</h2>
                    <p>Let's assume that we have a pure sine wave, which is our signal. It repeats itself at some point, where it actually completes one cycle. How many cycles it realizes per second is the frequency. Both frequency and sampling rate are specified in Hz. The relationship between frequency and cycle is represented as follows:</p>
                    
                    <p>$$F = 50 \\Leftrightarrow 50 \\text{ cycles } 1 \\text{ sec}$$</p>
                    <p>$$1 \\text{ Cycle} = \\frac{1}{50} \\text{ sec}$$</p>
                    
                    <h2>Sampling Rate and Cycles</h2>
                    <p>In the case of sampling rate $F_s$, I would generate $F_s$ number of samples per second, and one cycle needs 0.02 second. This implies that $F_s/F$ samples are generated for one cycle.</p>
                    
                    <p>$$F_s \\text{ samples} \\Leftrightarrow 1 \\text{ second}$$</p>
                    <p>$$F_s/F \\text{ samples} \\Leftrightarrow 1 \\text{ cycle}$$</p>
                    
                    <h2>The Nyquist-Shannon Theorem</h2>
                    <p>If our sampling rate $F_s$ is equal to 100 Hz, we would have 2 samples per cycle. These samples look like low, high, low, high … The problem is that many different waves actually can pass through these samples, such as the sine waves of 150 Hz and 250 Hz. In other words, the samples can't characterize the waveform distinctively.</p>
                    
                    <p>This conclusion leads us to reach the Nyquist–Shannon rule, which says that we need <strong>more than 2 samples per cycle</strong> to capture a frequency correctly.</p>
                    
                    <p>$$\\frac{F_s}{F} > 2 \\Longleftrightarrow F_{max} < \\frac{F_s}{2}$$</p>
                    
                    <h2>Practical Implications</h2>
                    <p>These two inequalities tell us that if your sampling rate is 44 KHz, you are only able to capture the frequencies below 22 KHz. This proves that higher sampling rate covers higher frequencies.</p>
                    
                    <p>For example, to be able to hear someone's speech clearly, 8 KHz frequency is quite sufficient; more than that is not so required. In that case, we generally re-sample the audio records of human speech in 16 KHz.</p>
                    
                    <h3>Common Sampling Rates</h3>
                    <ul>
                        <li><strong>16 KHz:</strong> Human speech processing</li>
                        <li><strong>22 KHz:</strong> Maximum frequency captured at 44 KHz sampling</li>
                        <li><strong>44 KHz:</strong> CD-quality audio</li>
                        <li><strong>48 KHz:</strong> Professional audio recording</li>
                    </ul>
                    
                    <h2>Conclusion</h2>
                    <p>Understanding the relationship between sampling rate and frequency is fundamental in digital audio processing. The Nyquist-Shannon theorem provides the theoretical foundation for determining appropriate sampling rates for different audio applications, from speech recognition to high-fidelity music reproduction.</p>
                `
            },
            "understanding-neural-networks": {
                title: "Understanding Neural Networks",
                date: "October 28, 2025",
                tags: ["Deep Learning", "Neural Networks", "Fundamentals"],
                content: `
                    <h2>Introduction to Neural Networks</h2>
                    <p>Neural networks are the foundation of modern deep learning. This post covers the basics from perceptrons to deep architectures.</p>
                    
                    <h2>The Perceptron</h2>
                    <p>The simplest neural network unit is the perceptron, which takes multiple inputs and produces a single output through a weighted sum and activation function.</p>
                    
                    <h2>Backpropagation</h2>
                    <p>Backpropagation is the algorithm used to train neural networks by computing gradients of the loss with respect to the weights.</p>
                    
                    <h3>Key Steps</h3>
                    <ol>
                        <li>Forward pass: Compute predictions</li>
                        <li>Compute loss</li>
                        <li>Backward pass: Compute gradients</li>
                        <li>Update weights</li>
                    </ol>
                    
                    <h2>Activation Functions</h2>
                    <p>Common activation functions include ReLU, sigmoid, and tanh, each with different properties and use cases.</p>
                `
            }
        };

        // Get post ID from URL
        const urlParams = new URLSearchParams(window.location.search);
        const postId = urlParams.get('id');

        // Load post content
        if (postId && blogPostsContent[postId]) {
            const post = blogPostsContent[postId];
            document.getElementById('postTitle').textContent = post.title;
            document.getElementById('postDate').textContent = post.date;
            document.getElementById('postContent').innerHTML = post.content;
            document.title = `${post.title} - Goktug Guvercin`;

            // Add tags
            const tagsContainer = document.getElementById('postTags');
            post.tags.forEach(tag => {
                const tagElement = document.createElement('span');
                tagElement.className = 'tag';
                tagElement.textContent = tag;
                tagsContainer.appendChild(tagElement);
            });
        } else {
            // Post not found
            document.getElementById('postTitle').textContent = 'Post Not Found';
            document.getElementById('postDate').textContent = '';
            document.getElementById('postContent').innerHTML = '<p>Sorry, this blog post could not be found.</p>';
        }
    </script>
</body>
</html>