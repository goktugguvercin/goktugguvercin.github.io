<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Blog Post - Goktug Guvercin</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&family=Playfair+Display:ital,wght@0,400;0,600;0,700;1,400&display=swap" rel="stylesheet">

    <script src="https://cdn.tailwindcss.com"></script>
    <script>
      tailwind.config = {
        theme: {
          extend: {
            colors: {
              cream: '#f5ebe0',
              gold: '#d4a373',
              dark: '#2c2c2c',
              sand: '#ede5d8',
              gray: { primary: '#5a5a5a', secondary: '#666666' }
            },
            fontFamily: {
              sans: ['Inter', 'sans-serif'],
              serif: ['Playfair Display', 'serif'],
            },
            backgroundImage: {
              'warm-radial': 'radial-gradient(circle at 15% 50%, rgba(212, 163, 115, 0.08) 0%, transparent 25%), radial-gradient(circle at 85% 30%, rgba(26, 115, 232, 0.05) 0%, transparent 25%)',
            }
          }
        }
      }
    </script>
    
    <!-- MathJax for LaTeX rendering -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true
            }
        };
    </script>
    
    <style>
        html { scroll-behavior: smooth; }
        ::-webkit-scrollbar { width: 8px; }
        ::-webkit-scrollbar-track { background: #f5ebe0; }
        ::-webkit-scrollbar-thumb { background: #d4a373; border-radius: 4px; }
        ::-webkit-scrollbar-thumb:hover { background: #c29365; }

        .fade-in { animation: fadeIn 0.5s ease-out forwards; }
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }

        .post-content {
            background: linear-gradient(135deg, #faf7f2 0%, #e8dcc8 100%);
            border: 2px solid rgba(212, 163, 115, 0.7);
            box-shadow: 0 6px 20px rgba(0, 0, 0, 0.12);
        }

        .post-content h2 {
            color: #1a1a1a;
            margin-top: 2rem;
            margin-bottom: 1rem;
            font-size: 1.8rem;
            font-weight: 700;
        }

        .post-content h3 {
            color: #2c2c2c;
            margin-top: 1.5rem;
            margin-bottom: 0.75rem;
            font-size: 1.4rem;
            font-weight: 600;
        }

        .post-content p {
            margin-bottom: 1.2rem;
            line-height: 1.8;
        }

        .post-content code {
            background-color: #f5f5f5;
            padding: 0.2rem 0.4rem;
            border-radius: 4px;
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
        }

        .post-content pre {
            background-color: #f5f5f5;
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin: 1.5rem 0;
        }

        .post-content pre code {
            background-color: transparent;
            padding: 0;
        }

        .post-content img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 1.5rem 0;
        }

        .post-content ul, .post-content ol {
            margin: 1rem 0;
            padding-left: 2rem;
        }

        .post-content li {
            margin-bottom: 0.5rem;
        }

        .back-link {
            color: #2c2c2c;
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
        }

        .back-link:hover {
            color: #d4a373;
        }

        .tag {
            background-color: transparent;
            border: 1px solid #d4a373;
            color: #8c6b4f;
            padding: 0.5rem 1.25rem;
            border-radius: 9999px;
            font-size: 0.875rem;
            font-weight: 500;
            cursor: default;
            transition: all 0.2s ease;
        }
        
        .tag:hover {
            background-color: #d4a373;
            color: white;
            transform: translateY(-2px);
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
        }
    </style>
</head>
<body class="bg-cream bg-warm-radial bg-fixed min-h-screen text-dark font-sans antialiased pb-16">

    <nav class="fixed top-0 left-0 w-full z-50 bg-white/85 backdrop-blur-md border-b border-gold/30 py-4">
        <div class="max-w-[900px] mx-auto px-8 flex justify-center gap-10">
            <a href="index.html" class="nav-btn text-gray-primary hover:text-gold relative font-serif text-lg font-semibold py-2 transition-colors duration-300">
                About
                <span class="absolute bottom-0 left-0 h-[2px] bg-gold w-0 transition-all duration-300"></span>
            </a>
            <a href="projects.html" class="nav-btn text-gray-primary hover:text-gold relative font-serif text-lg font-semibold py-2 transition-colors duration-300">
                Projects
                <span class="absolute bottom-0 left-0 h-[2px] bg-gold w-0 transition-all duration-300"></span>
            </a>
            <a href="blog.html" class="nav-btn text-gold relative font-serif text-lg font-semibold py-2 transition-colors duration-300">
                Blog
                <span class="absolute bottom-0 left-0 h-[2px] bg-gold w-full transition-all duration-300"></span>
            </a>
        </div>
    </nav>

    <div class="mt-[120px] w-full max-w-[900px] mx-auto px-6 md:px-8 fade-in">
        
        <a href="blog.html" class="back-link mb-8 inline-block">
            <span>←</span>
            <span>Back to Blog</span>
        </a>
        
        <article>
            <header class="mb-8">
                <div class="text-sm uppercase tracking-wider text-gold font-bold mb-3" id="postDate">Loading...</div>
                <h1 class="font-serif text-4xl md:text-5xl font-bold text-dark mb-6" id="postTitle">Loading...</h1>
            </header>

            <div class="post-content p-8 md:p-12 rounded-[20px] text-dark/90 text-lg" id="postContent">
                <!-- Blog post content will be loaded here -->
            </div>

            <div class="flex flex-wrap gap-3 mt-8" id="postTags">
                <!-- Tags will be loaded here -->
            </div>
        </article>

        <footer class="mt-16 text-center text-sm text-gray-secondary font-serif opacity-70">
            <p>© 2025 Goktug Guvercin. All rights reserved.</p>
        </footer>
    </div>

    <script>
        // Blog posts content database
        const blogPostsContent = {
            "transformers-attention-mechanism": {
                title: "Transformers and Attention Mechanism",
                date: "November 10, 2025",
                tags: ["Machine Learning", "NLP", "Transformers"],
                content: `
                    <h2>Introduction</h2>
                    <p>The transformer architecture has revolutionized natural language processing and many other domains of machine learning. In this post, we'll explore the key innovation that makes transformers so powerful: the attention mechanism.</p>
                    
                    <h2>What is Attention?</h2>
                    <p>At its core, attention allows a model to focus on relevant parts of the input when processing each element. Unlike traditional sequential models like RNNs, transformers can attend to any part of the sequence directly.</p>
                    
                    <h3>The Attention Formula</h3>
                    <p>The scaled dot-product attention is computed as:</p>
                    <p>$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$</p>
                    
                    <p>Where $d_k$ is the dimension of the key vectors. The division by $\\sqrt{d_k}$ prevents the dot products from becoming too large.</p>
                    
                    <h2>Key Components</h2>
                    <ul>
                        <li><strong>Query (Q):</strong> What we're looking for</li>
                        <li><strong>Key (K):</strong> What we're matching against</li>
                        <li><strong>Value (V):</strong> The actual information we retrieve</li>
                    </ul>
                    
                    <h3>Multi-Head Attention</h3>
                    <p>Instead of performing a single attention function, multi-head attention runs $h$ attention mechanisms in parallel:</p>
                    <p>$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$</p>
                    <p>where each head is: $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$</p>
                    
                    <h2>Example: Adding an Image</h2>
                    <p>You can easily add images to your blog posts. Here's the syntax:</p>
                    <pre><code>&lt;img src="path/to/your/image.png" alt="Description"&gt;</code></pre>
                    
                    <h2>Conclusion</h2>
                    <p>Understanding the attention mechanism is crucial for working with modern NLP models. This foundational concept enables transformers to process sequences efficiently and effectively.</p>
                `
            },
            "sampling-frequency-audio": {
                title: "Sampling and Frequency in Audio Models",
                date: "November 5, 2025",
                tags: ["Audio Processing", "Signal Processing", "Digital Audio"],
                content: `
                    <h2>From Analog to Digital Audio</h2>
                    <p>Sound in the real world comprises pressure waves in the air. It is smooth, and continuously changes. A microphone converts those pressure waves into the voltage that changes smoothly over time. This electrical signal is called analog audio, and its waveform is roughly proportional to the original sound pressure.</p>
                    
                    <p>This analog signal is then turned into a digital audio record by an A/D converter. These converters look at analog signal many times per second, measures how high or low that voltage is and converts measured voltage (amplitude) into a number. In the final stage, these values are quantized into the nearest digital value, as its bit depth allows. The whole process is named as sampling, and the number of samples per second refers to the sample rate (Hz). Higher sampling rate captures higher frequencies and more detail about the original signal.</p>
                    
                    <p>At the end, this digital audio is stored in files such as WAV or AIFF, which typically contain uncompressed PCM audio. MP3 format also stores digital audio, but it uses lossy compression to reduce file size by discarding some audio information.</p>
                    
                    <h2>Understanding Frequency and Cycles</h2>
                    <p>Consider a pure sine wave as our audio signal. By definition, it is a periodic wave, meaning that it repeats a specific pattern, which is called a cycle. The number of cycles it completes per second is its frequency $F$, measured in Hertz (Hz).</p>
                    
                    <p>$$F = 50 \\Leftrightarrow 50 \\text{ cycles } 1 \\text{ sec}$$</p>
                    <p>$$1 \\text{ Cycle} = \\frac{1}{50} \\text{ sec}$$</p>
                    
                    <h2>Sampling Rate and Cycles</h2>
                    <p>When we digitize this signal, we choose a specific sampling rate $F_s$. It defines how many samples we will record from it per second. If one cycle needs 0.02 seconds to be completed, $F_s/F$ samples are measured for that cycle.</p>
                    
                    <p>$$F_s \\text{ samples} \\Leftrightarrow 1 \\text{ second}$$</p>
                    <p>$$F_s/F \\text{ samples} \\Leftrightarrow 1 \\text{ cycle}$$</p>
                    
                    <p>What if we set our sampling rate to be exactly twice the frequency $F$ (i.e., $F_s = 100$)? At this point, one important problem may occur. Based on our formula, this would give us 2 samples per cycle.</p>
                    
                    <p>These two samples might just capture the peak and the trough (a "high, low, high, low..." pattern). The issue is that many different waves, such as sine waves with the frequencies of $F$, $3F$, and $5F$, can all pass through these exact same points. In other words, the samples fail to characterize the original waveform distinctively.</p>
                    
                    <h2>Nyquist-Shannon Sampling Theorem</h2>
                    <p>Nyquist–Shannon sampling theorem states that we need <strong>more than 2 samples per cycle</strong> to capture a frequency correctly. We can express this with an inequality:</p>
                    
                    <p>$$\\frac{F_s}{F} > 2 \\Longleftrightarrow F_{max} < \\frac{F_s}{2}$$</p>
                    
                    <p>The definition of $F_{max}$ states that the highest frequency we can accurately capture must be <em>less than half</em> of our sampling rate $F_s$. This proves that higher sampling rate covers higher frequencies. For example, to be able to hear someone's speech clearly, 8 KHz frequency is quite sufficient; more than that is not so required. In that case, we generally re-sample the audio records of human speech in 16 KHz.</p>
                    
                    <h3>Machine Learning Models and 16 KHz Standard</h3>
                    <p>Understanding the relationship between sampling rate and frequency is fundamental in digital audio processing. In both training and inference stages, the audio records are specifically re-sampled into 16 KHz, as a de-facto standard for speech recognition models. Whisper, HuBERT, Moonshine, and Wav2Vec 2.0 are several state-of-the-art ML models designed around 16 KHz audio.</p>
                    
                    <h2>Conclusion</h2>
                    <p>The Nyquist-Shannon theorem provides the theoretical foundation for determining appropriate sampling rates for different audio applications. By ensuring that our sampling rate is more than twice the highest frequency we want to capture, we can accurately represent audio signals digitally, whether for speech recognition, music reproduction, or other audio processing tasks.</p>
                `
            },
            "understanding-neural-networks": {
                title: "Understanding Neural Networks",
                date: "October 28, 2025",
                tags: ["Deep Learning", "Neural Networks", "Fundamentals"],
                content: `
                    <h2>Introduction to Neural Networks</h2>
                    <p>Neural networks are the foundation of modern deep learning. This post covers the basics from perceptrons to deep architectures.</p>
                    
                    <h2>The Perceptron</h2>
                    <p>The simplest neural network unit is the perceptron, which takes multiple inputs and produces a single output through a weighted sum and activation function.</p>
                    
                    <h2>Backpropagation</h2>
                    <p>Backpropagation is the algorithm used to train neural networks by computing gradients of the loss with respect to the weights.</p>
                    
                    <h3>Key Steps</h3>
                    <ol>
                        <li>Forward pass: Compute predictions</li>
                        <li>Compute loss</li>
                        <li>Backward pass: Compute gradients</li>
                        <li>Update weights</li>
                    </ol>
                    
                    <h2>Activation Functions</h2>
                    <p>Common activation functions include ReLU, sigmoid, and tanh, each with different properties and use cases.</p>
                `
            }
        };

        // Get post ID from URL
        const urlParams = new URLSearchParams(window.location.search);
        const postId = urlParams.get('id');

        // Load post content
        if (postId && blogPostsContent[postId]) {
            const post = blogPostsContent[postId];
            document.getElementById('postTitle').textContent = post.title;
            document.getElementById('postDate').textContent = post.date;
            document.getElementById('postContent').innerHTML = post.content;
            document.title = `${post.title} - Goktug Guvercin`;

            // Add tags
            const tagsContainer = document.getElementById('postTags');
            post.tags.forEach(tag => {
                const tagElement = document.createElement('span');
                tagElement.className = 'tag';
                tagElement.textContent = tag;
                tagsContainer.appendChild(tagElement);
            });
        } else {
            // Post not found
            document.getElementById('postTitle').textContent = 'Post Not Found';
            document.getElementById('postDate').textContent = '';
            document.getElementById('postContent').innerHTML = '<p>Sorry, this blog post could not be found.</p>';
        }
    </script>
</body>
</html>